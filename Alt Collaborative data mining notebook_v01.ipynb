{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change log\n",
    "\n",
    "Could be a good idea to write what we changed and when/which version we are on now so we don't mix things up?\n",
    "Or maybe not, do as you wish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- version 01\n",
    "\n",
    "Magnus - did a thing\n",
    "\n",
    "Emily - Fixed the normalization part + imputer + other stuff\n",
    "\n",
    "- version 02\n",
    "\n",
    "Dana - introduced the one-hot encoding for preprocessing + added confusion matrix. Combining the models made by us all with the assignment detailes and providing the flow.\n",
    "\n",
    "Magnus - started on the codes for svm and logistic regression (changed from linear regression which is only for regression, not classification) + added the lines for the model to be saved and exported into a file\n",
    "\n",
    "- version 03 -\n",
    "\n",
    "magnus - RandomSearchCV is implemented for all classification models, all are working\n",
    "\n",
    "- alt version 01 -\n",
    "\n",
    "magnus - if the data set columns has to stay the same, this is a quick fix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions \n",
    "Classification predictive modeling is the task of approximating a mapping function (f) from input variables (X) to discrete output variables (y). From the Input - till the output is one model, which may use different classification algorithms.\n",
    "\n",
    "Classification algorithms (techniques) in supervised learning include: logistic regression, decision tree, random forest, gradient-boosted tree, multilayer perceptron, one-vs-rest, and Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment: \n",
    "\n",
    "WHAT: The objective is to build a system for predicting heart disease and its intensity, based on observed attributes of a patient. You will have to decide which attributes to use, which classifiers to test, and eventually\n",
    "which classifier to deploy.\n",
    "\n",
    "HOW: You can use scikit-learn for pre-processing your data sources, building several data mining\n",
    "models (at least three) for heart disease classification, and eventually creating a diagnosis tool for\n",
    "heart disease that can be used by practitioners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1. Using Cleveland dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleveland dataset is the most completed (only one missing value). Therefore it has been chosen for the building the classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data and change ? to nan-values\n",
    "cle = pd.read_csv('processed.cleveland.csv', keep_default_na=False, na_values=[\"?\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjusting column names and NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change num to goal\n",
    "cle.rename(columns={'num': 'goal'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing NaN-values with most frequent\n",
    "from sklearn.impute import SimpleImputer\n",
    "replc = SimpleImputer(strategy= 'most_frequent')\n",
    "replc = replc.fit_transform(cle)\n",
    "cle = pd.DataFrame(replc, columns=cle.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>goal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>45.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>68.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>57.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "0    63.0  1.0  1.0     145.0  233.0  1.0      2.0    150.0    0.0      2.3   \n",
       "1    67.0  1.0  4.0     160.0  286.0  0.0      2.0    108.0    1.0      1.5   \n",
       "2    67.0  1.0  4.0     120.0  229.0  0.0      2.0    129.0    1.0      2.6   \n",
       "3    37.0  1.0  3.0     130.0  250.0  0.0      0.0    187.0    0.0      3.5   \n",
       "4    41.0  0.0  2.0     130.0  204.0  0.0      2.0    172.0    0.0      1.4   \n",
       "..    ...  ...  ...       ...    ...  ...      ...      ...    ...      ...   \n",
       "298  45.0  1.0  1.0     110.0  264.0  0.0      0.0    132.0    0.0      1.2   \n",
       "299  68.0  1.0  4.0     144.0  193.0  1.0      0.0    141.0    0.0      3.4   \n",
       "300  57.0  1.0  4.0     130.0  131.0  0.0      0.0    115.0    1.0      1.2   \n",
       "301  57.0  0.0  2.0     130.0  236.0  0.0      2.0    174.0    0.0      0.0   \n",
       "302  38.0  1.0  3.0     138.0  175.0  0.0      0.0    173.0    0.0      0.0   \n",
       "\n",
       "     slope   ca  thal  goal  \n",
       "0      3.0  0.0   6.0   0.0  \n",
       "1      2.0  3.0   3.0   2.0  \n",
       "2      2.0  2.0   7.0   1.0  \n",
       "3      3.0  0.0   3.0   0.0  \n",
       "4      1.0  0.0   3.0   0.0  \n",
       "..     ...  ...   ...   ...  \n",
       "298    2.0  0.0   7.0   1.0  \n",
       "299    2.0  2.0   7.0   2.0  \n",
       "300    2.0  1.0   7.0   3.0  \n",
       "301    2.0  1.0   3.0   1.0  \n",
       "302    1.0  0.0   3.0   0.0  \n",
       "\n",
       "[303 rows x 14 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separating features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everything except goal\n",
    "cle_features = cle.drop(['goal'], axis=1)\n",
    "\n",
    "# Only goal\n",
    "cle_label = cle['goal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_NoNorm = cle_features.drop(['age', 'trestbps', 'chol', 'oldpeak', 'thalach'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = 'age', 'trestbps', 'chol', 'thalach', 'oldpeak', \n",
    "X = cle_features[[c for c in cle_features if c in numerical]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "feat_norm = scaler.fit_transform(X)\n",
    "feat_norm = pd.DataFrame(feat_norm, columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>thalach</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>exang</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.948726</td>\n",
       "      <td>0.757525</td>\n",
       "      <td>-0.264900</td>\n",
       "      <td>0.017197</td>\n",
       "      <td>1.087338</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.392002</td>\n",
       "      <td>1.611220</td>\n",
       "      <td>0.760415</td>\n",
       "      <td>-1.821905</td>\n",
       "      <td>0.397182</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.392002</td>\n",
       "      <td>-0.665300</td>\n",
       "      <td>-0.342283</td>\n",
       "      <td>-0.902354</td>\n",
       "      <td>1.346147</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.932564</td>\n",
       "      <td>-0.096170</td>\n",
       "      <td>0.063974</td>\n",
       "      <td>1.637359</td>\n",
       "      <td>2.122573</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.489288</td>\n",
       "      <td>-0.096170</td>\n",
       "      <td>-0.825922</td>\n",
       "      <td>0.980537</td>\n",
       "      <td>0.310912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>-1.046013</td>\n",
       "      <td>-1.234430</td>\n",
       "      <td>0.334813</td>\n",
       "      <td>-0.770990</td>\n",
       "      <td>0.138373</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>1.502821</td>\n",
       "      <td>0.700612</td>\n",
       "      <td>-1.038723</td>\n",
       "      <td>-0.376896</td>\n",
       "      <td>2.036303</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.283813</td>\n",
       "      <td>-0.096170</td>\n",
       "      <td>-2.238149</td>\n",
       "      <td>-1.515388</td>\n",
       "      <td>0.138373</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>0.283813</td>\n",
       "      <td>-0.096170</td>\n",
       "      <td>-0.206864</td>\n",
       "      <td>1.068113</td>\n",
       "      <td>-0.896862</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>-1.821745</td>\n",
       "      <td>0.359134</td>\n",
       "      <td>-1.386944</td>\n",
       "      <td>1.024325</td>\n",
       "      <td>-0.896862</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          age  trestbps      chol   thalach   oldpeak  sex   cp  fbs  restecg  \\\n",
       "0    0.948726  0.757525 -0.264900  0.017197  1.087338  1.0  1.0  1.0      2.0   \n",
       "1    1.392002  1.611220  0.760415 -1.821905  0.397182  1.0  4.0  0.0      2.0   \n",
       "2    1.392002 -0.665300 -0.342283 -0.902354  1.346147  1.0  4.0  0.0      2.0   \n",
       "3   -1.932564 -0.096170  0.063974  1.637359  2.122573  1.0  3.0  0.0      0.0   \n",
       "4   -1.489288 -0.096170 -0.825922  0.980537  0.310912  0.0  2.0  0.0      2.0   \n",
       "..        ...       ...       ...       ...       ...  ...  ...  ...      ...   \n",
       "298 -1.046013 -1.234430  0.334813 -0.770990  0.138373  1.0  1.0  0.0      0.0   \n",
       "299  1.502821  0.700612 -1.038723 -0.376896  2.036303  1.0  4.0  1.0      0.0   \n",
       "300  0.283813 -0.096170 -2.238149 -1.515388  0.138373  1.0  4.0  0.0      0.0   \n",
       "301  0.283813 -0.096170 -0.206864  1.068113 -0.896862  0.0  2.0  0.0      2.0   \n",
       "302 -1.821745  0.359134 -1.386944  1.024325 -0.896862  1.0  3.0  0.0      0.0   \n",
       "\n",
       "     exang  slope   ca  thal  \n",
       "0      0.0    3.0  0.0   6.0  \n",
       "1      1.0    2.0  3.0   3.0  \n",
       "2      1.0    2.0  2.0   7.0  \n",
       "3      0.0    3.0  0.0   3.0  \n",
       "4      0.0    1.0  0.0   3.0  \n",
       "..     ...    ...  ...   ...  \n",
       "298    0.0    2.0  0.0   7.0  \n",
       "299    0.0    2.0  2.0   7.0  \n",
       "300    1.0    2.0  1.0   7.0  \n",
       "301    0.0    2.0  1.0   3.0  \n",
       "302    0.0    1.0  0.0   3.0  \n",
       "\n",
       "[303 rows x 13 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cle_feat = pd.concat([feat_norm,feat_NoNorm], axis=1)\n",
    "cle_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train - test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(cle_feat, cle_label, test_size=0.3, random_state=321, stratify=cle_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three models to test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment\n",
    "\n",
    "Task 3: (CLASSIFIERS) Select at least three classifiers\n",
    "You may choose any set of classifiers for this assignment. \n",
    "\n",
    "Make sure you select at least three and\n",
    "provide some reasoning of your choice. The classification models should be built using either all\n",
    "attributes or a subset of the attributes. Similarly, you may also want to exclude some of the\n",
    "datasets. If you decide to use a subset of attributes or exclude any dataset or part of a dataset,\n",
    "please justify your choice carefully. Note that your justification should be motivated either\n",
    "medically or based on an observation you have made when preprocessing the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Random forest classifier + Random Search Cross-validation\n",
    "\n",
    "By using RandomSearchCV we can find which parameters that are most efficient to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create an empty classifier\n",
    "rfclf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The parameters are:\n",
      "\n",
      "{'bootstrap': True,\n",
      " 'ccp_alpha': 0.0,\n",
      " 'class_weight': None,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': None,\n",
      " 'max_features': 'auto',\n",
      " 'max_leaf_nodes': None,\n",
      " 'max_samples': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_impurity_split': None,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 100,\n",
      " 'n_jobs': None,\n",
      " 'oob_score': False,\n",
      " 'random_state': None,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "print('The parameters are:\\n')\n",
    "\n",
    "# Get a look att all the parameters that we can change\n",
    "pprint(rfclf.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "From research, the following parameters are usually what is included in the search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': [True, False],\n",
      " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
      " 'max_features': ['auto', 'sqrt', 'log2'],\n",
      " 'min_samples_leaf': [1, 2, 5, 10, 15],\n",
      " 'min_samples_split': [2, 5, 10, 15, 20],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n"
     ]
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt', 'log2']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10, 15, 20]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 5, 10, 15]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid_rf = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "pprint(random_grid_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  2.4min\n"
     ]
    }
   ],
   "source": [
    "# Now we search for best hyperparameters, we already have the default base model \"clf\"\n",
    "\n",
    "# Random search of parameters, using 5 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rfclf, param_distributions = random_grid_rf, n_iter = 100, cv = 5, verbose=2, random_state=1, n_jobs = -1)\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "# This will take some minutes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the best parameters\n",
    "best_n_estimators = rf_random.best_params_['n_estimators']\n",
    "best_min_samples_split = rf_random.best_params_['min_samples_split']\n",
    "best_min_samples_leaf = rf_random.best_params_['min_samples_leaf']\n",
    "best_max_features = rf_random.best_params_['max_features']\n",
    "best_max_depth = rf_random.best_params_['max_depth']\n",
    "best_bootstrap = rf_random.best_params_['bootstrap']\n",
    "\n",
    "# Show the best parameters\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we use the best parameters to get a score.\n",
    "rfclf_best = RandomForestClassifier(n_estimators=best_n_estimators, max_depth=best_max_depth, min_samples_split=best_min_samples_split, min_samples_leaf=best_min_samples_leaf, max_features=best_max_features, bootstrap=best_bootstrap, random_state=1)\n",
    "rfclf_best.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf =rfclf_best.predict(X_test)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_classify = [45, 1, 2, 120, 155, 1, 0, 140, 0, 1.5, 3, 0, 3.0]\n",
    "colnames = X_test.columns\n",
    "sample = pd.DataFrame(data = [data_to_classify], columns = colnames)\n",
    "sample\n",
    "prediction = model.predict(sample)\n",
    "print(\"The Patient has a predicted risk for Heart disease of \", prediction[0])\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) SVM + Random Search CV\n",
    "\n",
    "One-vs-one is most frequently used in logistic regression for multi-class classification.\n",
    "\n",
    "Same process as the random forest but for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC \n",
    "# train the model on train set \n",
    "svmclf = SVC(decision_function_shape='ovo')\n",
    "\n",
    "print('The parameters are:\\n')\n",
    "pprint(svmclf.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code comes from:\n",
    "# https://cambridgecoding.wordpress.com/2016/05/16/expanding-your-machine-learning-toolkit-randomized-search-computational-budgets-and-new-algorithms-2/\n",
    "\n",
    "# Designate distributions to sample hyperparameters from \n",
    "np.random.seed(123)\n",
    "g_range = np.random.uniform(0.0, 0.3, 5).astype(float)\n",
    "C_range = np.random.normal(1, 0.1, 5).astype(float)\n",
    "\n",
    "# Check that gamma>0 and C>0 \n",
    "C_range[C_range < 0] = 0.0001\n",
    "\n",
    "random_grid_svm = [{'C': list(C_range), 'gamma': list(g_range), 'kernel': ['linear'],'class_weight':['balanced', None]},\n",
    "    {'C': list(C_range), 'gamma': list(g_range), 'kernel': ['rbf'],'class_weight':['balanced', None]},\n",
    "    {'C': list(C_range), 'gamma': list(g_range), 'kernel': ['poly'],'class_weight':['balanced', None]},\n",
    "    {'C': list(C_range), 'gamma': list(g_range), 'kernel': ['sigmoid'],'class_weight':['balanced', None]}]\n",
    "\n",
    "print('The random grid parameters are:\\n')\n",
    "pprint(random_grid_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run randomized search\n",
    "svm_random = RandomizedSearchCV(estimator = svmclf, param_distributions = random_grid_svm, n_iter=80, cv =5, verbose=2, n_jobs = -1)\n",
    "svm_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify optimal hyperparameter values\n",
    "best_gamma  = svm_random.best_params_['gamma']\n",
    "best_C      = svm_random.best_params_['C']\n",
    "best_kernel = svm_random.best_params_['kernel']\n",
    "best_class_weight = svm_random.best_params_['class_weight']\n",
    "\n",
    "print(\"The best performing gamma value is: {:5.2f}\".format(best_gamma))\n",
    "print(\"The best performing C value is: {:5.2f}\".format(best_C))\n",
    "print(\"The best performing kernel is: {}\".format(best_kernel))\n",
    "print(\"The best performing class weight is: {}\".format(best_class_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmclf_best = SVC(kernel=best_kernel, decision_function_shape='ovo', C=best_C, gamma=best_gamma, class_weight=best_class_weight)\n",
    "svmclf_best.fit(X_train, y_train)\n",
    "y_pred_svm = svmclf_best.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Logistic* regression + Random Search CV\n",
    "#### ! Changed to logistic bc linear regression seems to only be used for regressions and not classifications, while logistic can be used for both.\n",
    "\n",
    "One-vs-rest is most frequently used in logistic regression for multiclassification\n",
    "Same process as the random forest but for linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lrclf = LogisticRegression(multi_class='ovr', max_iter=2000)\n",
    "# lrclf.fit(X_train, y_train)\n",
    "print('The parameters are:\\n')\n",
    "pprint(lrclf.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C = np.logspace(0, 4, num=10)\n",
    "# penalty = ['l1', 'l2']\n",
    "# solver = ['liblinear', 'saga']\n",
    "# hyperparameters = dict(C=C, penalty=penalty, solver=solver)\n",
    "\n",
    "# randomizedsearch = RandomizedSearchCV(logistic, hyperparameters)\n",
    "# best_model_random = randomizedsearch.fit(features, target)\n",
    "# print(best_model_random.best_estimator_)\n",
    "\n",
    "\n",
    "penalty = ['none', 'l1', 'l2', 'elasticnet']\n",
    "solver = ['newton-cg', 'lbfgs', 'liblinear', 'saga']\n",
    "C_lr = np.logspace(0, 4, num=10)\n",
    "\n",
    "random_grid_lr = {'penalty': penalty,\n",
    "                 'solver': solver,\n",
    "                 'C': C_lr}\n",
    "print('The random grid parameters are:\\n')\n",
    "pprint(random_grid_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_random = RandomizedSearchCV(estimator = lrclf, param_distributions = random_grid_lr, n_iter=100, cv =5, verbose=2, n_jobs = -1)\n",
    "lr_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_penalty  = lr_random.best_params_['penalty']\n",
    "best_solver      = lr_random.best_params_['solver']\n",
    "best_C_lr = lr_random.best_params_['C']\n",
    "\n",
    "print(\"The best performing penalty is: {}\".format(best_penalty))\n",
    "print(\"The best performing solver is: {}\".format(best_solver))\n",
    "print(\"The best performing C is: {}\".format(best_C_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrclf_best = LogisticRegression(multi_class='ovr', penalty=12, C=0.9948482279060615, solver=lbfgs, max_iter=2000)\n",
    "lrclf_best.fit(X_train, y_train)\n",
    "y_pred_lr = lrclf_best.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "X_train, X_test, y_train, y_test = train_test_split(cle_feat, \n",
    "                                                    cle_label, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=321, # Match the random state with above\n",
    "                                                    stratify=cle_label)\n",
    "model=lrclf_best # Change to best performing model here\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "hd_model = make_pipeline( # Remove since this is without one-hot?\n",
    "    ce.OneHotEncoder(use_cat_names=True), \n",
    "    model) \n",
    "print ('the model selecetd is ', model)\n",
    "hd_model.fit(X_train, y_train)\n",
    "dump(hd_model,'finalized_hd_model.pkl')\n",
    "y_pred = hd_model.predict(X_test)\n",
    "print (y_pred)\n",
    "result = hd_model.score(X_test, y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model 2. Using all datasets combined together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the datasets  were combined in one data set. \n",
    "Several datasets have been missing data in several rows. Those rows have been removed originally by code (dropna), then just manually, with same outcomes after classification.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = pd.read_csv('all.csv', delimiter = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = all.drop(['fbs','ca','slope','thal'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = all.replace('?', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all.dropna(thresh=2, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all=all.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all=all.fillna(all.median(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all.exang.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all['Sex'].replace({0:'f',1:'m'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all['Cp'].replace({1:'typ_ang',2: 'atyp_ang',3: 'non_ang',4: 'asym'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all['restecg'].replace({'0.0':'norm', '1.0':'ST_T_abnorm','2.0':'l_vent_hyp', 0.0:'norm'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all['exang'].replace({'1.0':'yes','0.0':'no', 0.0:'no'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = all.drop(all.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = all.drop(all.columns[[-1]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = all.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_df = pd.DataFrame(enc.fit_transform(x[['Sex','Cp','restecg','exang','Country']]).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_df.head(672)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_df.columns = [\"female\", \"male\", \"asym\", \"atyp_ang\", 'non_ang',\n",
    "                  'typ_ang','ST_T_abnorm','l_vent_hyp','restecg_norm',\n",
    "                  'exang_no', 'exang_yes','cali','cle','hung','switz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.drop(['Sex','Cp','restecg','exang','Country'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "feat_norm = scaler.fit_transform(x)\n",
    "feat_norm = pd.DataFrame(feat_norm, columns=x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.concat([x, enc_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=123, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=250, random_state=123, min_samples_leaf=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_all=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_cv_score = cross_val_score(clf, x, y, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"=== Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test, y_pred_all))\n",
    "print('\\n')\n",
    "print(\"=== Classification Report ===\")\n",
    "print(classification_report(y_test, y_pred_all))\n",
    "print('\\n')\n",
    "print(\"=== All AUC Scores ===\")\n",
    "print(rfc_cv_score)\n",
    "print('\\n')\n",
    "print(\"=== Mean AUC Score ===\")\n",
    "print(\"Mean AUC Score - Random Forest: \", rfc_cv_score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outcomes for the rf classifier are lower than with the Cleveland data sets. \n",
    "As the assignments states the model should help as a  diagnosis tool for heart disease that can be used by practitioners, building a model using all datasets has not been chosen as the final one (the decision was also supported by the voting in the group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3. Whether the patient has heart disease. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification will be higher if we would only predict whether each patient has any heart disease. \n",
    "In this case the goal will display: 0 = absence 1,2,3,4 - presence of the heart disease. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cle_hd = pd.DataFrame(replc, columns=cle.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cle_hd['sex'].replace({0:'f',1:'m'}, inplace=True)\n",
    "cle_hd['cp'].replace({1:'typ_ang',2: 'atyp_ang',3: 'non_ang',4: 'asym'}, inplace=True)\n",
    "cle_hd['fbs'].replace({1:'true', 0:'false'}, inplace=True)\n",
    "cle_hd['restecg'].replace({0:'norm', 1:'ST_T_abnorm',2:'l_vent_hyp'}, inplace=True)\n",
    "cle_hd['exang'].replace({1:'yes',0:'no'}, inplace=True)\n",
    "cle_hd['slope'].replace({1:'upslop',2:'flat',3:'downslop'}, inplace=True)\n",
    "cle_hd['thal'].replace({3.0:'normal',6.0:'fixed defect',7.0:'reversable defect'}, inplace=True)\n",
    "cle_hd['goal'].replace({4.0: 1, 3.0:1, 2.0:1, 0.0: 0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cle_hd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cle_hd.restecg.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cle_hd_features = cle_hd.drop(['goal'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cle_hd_label = cle_hd['goal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recheck\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc_df = pd.DataFrame(enc.fit_transform(cle_hd_features[['sex','cp','fbs','restecg','exang','slope']]).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_df.columns = [\"female\", \"male\", \"asym\", \"atyp_ang\", 'non_ang',\n",
    "                  'typ_ang','fbs_false','fbs_true','ST_T_abnorm','l_vent_hyp','restecg_norm',\n",
    "                  'exang_no', 'exang_yes','upslop','flat','downslop','thal_fix','thal_normal','thal_reversed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cle_hd_features = pd.concat([cle_hd_features, enc_df], axis=1)\n",
    "cle_hd_features = cle_hd_features.drop(['sex','cp','fbs','restecg','exang','slope', 'thal'], axis=1)\n",
    "cle_hd_features['ca'] = pd.to_numeric(cle_hd_features['ca'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recheck\n",
    "feat_onehot_hd = cle_hd_features.drop(['age', 'trestbps', 'chol', 'oldpeak', 'thalach'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_hd = 'age', 'trestbps', 'chol', 'thalach', 'oldpeak', \n",
    "X_hd = cle_hd_features[[c for c in cle_features if c in numerical]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_hd = StandardScaler()\n",
    "feat_norm_hd = scaler_hd.fit_transform(X_hd)\n",
    "feat_norm_hd = pd.DataFrame(feat_norm_hd, columns=X_hd.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cle_feat_hd = pd.concat([feat_norm_hd,feat_onehot_hd], axis=1)\n",
    "cle_feat_hd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_hd, X_test_hd, y_train_hd, y_test_hd = train_test_split(cle_feat_hd, cle_hd_label, test_size=0.3, random_state=123, stratify=cle_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfclf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(rfclf.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfclf.fit(X_train_hd, y_train_hd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_hd=rfclf.predict(X_test_hd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test_hd, y_pred_hd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_hd_cv_score = cross_val_score(rfclf, cle_feat_hd, cle_hd_label, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Confusion Matrix ===\")\n",
    "print(confusion_matrix(y_test_hd, y_pred_hd))\n",
    "print('\\n')\n",
    "print(\"=== Classification Report ===\")\n",
    "print(classification_report(y_test_hd, y_pred_hd))\n",
    "print('\\n')\n",
    "print(\"=== All AUC Scores ===\")\n",
    "print(rfc_hd_cv_score)\n",
    "print('\\n')\n",
    "print(\"=== Mean AUC Score ===\")\n",
    "print(\"Mean AUC Score - Random Forest: \", rfc_hd_cv_score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Although this classification model represents the highest possible impact on the practicioner work.  \n",
    "### According to the model evaluation 82.8% of patients can be diagnosed as not having any heart condition, therefore the practitioner may pay attention to the 16.2% of patients, requiring more specified diagnose. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The assignment description requires both:\n",
    "     1. to build a system for predicting heart disease and its intensity\n",
    "     2. creating a diagnosis tool for heart disease that can be used by practitioners.\n",
    "    \n",
    "## The group has came to a conclusion to use the best performing model 1, with the best performing classifier Logistic* regression + Random Search CV. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
